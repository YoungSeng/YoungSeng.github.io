---
title: "Wavsyncswap: End-To-End Portrait-Customized Audio-Driven Talking Face Generation"
collection: publications
permalink: /publication/2023-05-05
date: 2023-05-05
venue: 'ICASSP'
citation: 'Weihong Bao, Liyang Chen, Chaoyong Zhou, <b>Sicheng Yang</b>, Zhiyong Wu. (2023). &quot;Wavsyncswap: End-To-End Portrait-Customized Audio-Driven Talking Face Generation.&quot; <i>ICASSP</i>, 1-5, doi: 10.1109/ICASSP49357.2023.10094807.'
---



<!-- Recommended citation: Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1). -->

Audio-driven talking face with portrait customization enhances the flexibility of avatar applications for different scenarios, such as on-line meetings, mixed reality, and data generation. Among the existing methods, audio-driven talking face and face swapping are typically viewed as separate tasks that are cascaded to achieve the objective. Using state-of-the-art methods Wav2Lip and SimSwap for this purpose, we meet some issues: affected mouth synchronization, lost texture information, and slow inference speed. To resolve these issues, we propose an end-to-end model that combines the advantages of both approaches. Our approach generates highly-synchronized mouth with the aid of a pre-trained lip-sync discriminator. And identity information is provided by ArcFace and the ID injection module in the model because of its strong correlation with facial texture. Experimental results demonstrate that our method achieves lip-sync accuracy comparable to real synced videos, preserves more texture details than cascade methods, and alleviates the blurring of Wav2Lip. Also, our approach improves the inference speed. 

[Browse paper here](/files/Wavsyncswap_End-To-End_Portrait-Customized_Audio-Driven_Talking_Face_Generation.pdf)
