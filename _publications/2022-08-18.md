---
title: "Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion"
collection: publications
permalink: /publication/2022-08-18
date: 2022-08-18
venue: 'Interspeech'
citation: '<b>SiCheng Yang*</b>, Methawee Tantrawenith*, Haolin Zhuang*, Zhiyong Wu, Aolan Sun, Jianzong Wang, Ning Cheng, Huaizhen Tang, Xintao Zhao, Jie Wang, Helen Meng. (2022). &quot;Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion.&quot; <i>Interspeech</i>, 2553-2557, doi: 10.21437/Interspeech.2022-571.'
---

<!-- Recommended citation: Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1). -->

One-shot voice conversion (VC) with only a single target speaker's speech for reference has become a hot research topic. Existing works generally disentangle timbre, while information about pitch, rhythm and content is still mixed together. To perform one-shot VC effectively with further disentangling these speech components, we employ random resampling for pitch and content encoder and use the variational contrastive log-ratio upper bound of mutual information and gradient reversal layer based adversarial mutual information learning to ensure the different parts of the latent space containing only the desired disentangled representation during training. Experiments on the VCTK dataset show the model achieves state-of-the-art performance for one-shot VC in terms of naturalness and intellgibility. In addition, we can transfer characteristics of one-shot VC on timbre, pitch and rhythm separately by speech representation disentanglement. Our code, pre-trained models and demo are available at [this https URL](https://im1eon.github.io/IS2022-SRDVC/).


[Browse paper here](/files/2208_08757.pdf)
