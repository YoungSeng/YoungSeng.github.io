---
title: "The DiffuseStyleGesture+ entry to the GENEA Challenge 2023"
collection: publications
permalink: /publication/2023-08-05
date: 2023-08-05
venue: 'ICMI (Reproducibility Award)'
citation: '<b>Sicheng Yang*</b>, Haiwei Xue*, Zhensong Zhang, Minglei Li, Zhiyong Wu, Xiaofei Wu, Songcen Xu, Zonghong Dai. (2023). &quot;The DiffuseStyleGesture+ entry to the GENEA Challenge 2023.&quot; <i>ICMI</i>, 779â€“785, doi: 10.1145/3577190.3616114.'
---


<!-- Recommended citation: Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1). -->

In this paper, we introduce the DiffuseStyleGesture+, our solution for the Generation and Evaluation of Non-verbal Behavior for Embodied Agents (GENEA) Challenge 2023, which aims to foster the development of realistic, automated systems for generating conversational gestures. Participants are provided with a pre-processed dataset and their systems are evaluated through crowdsourced scoring. Our proposed model, DiffuseStyleGesture+, leverages a diffusion model to generate gestures automatically. It incorporates a variety of modalities, including audio, text, speaker ID, and seed gestures. These diverse modalities are mapped to a hidden space and processed by a modified diffusion model to produce the corresponding gesture for a given speech input. Upon evaluation, the DiffuseStyleGesture+ demonstrated performance on par with the top-tier models in the challenge, showing no significant differences with those models in human-likeness, appropriateness for the interlocutor, and achieving competitive performance with the best model on appropriateness for agent speech. This indicates that our model is competitive and effective in generating realistic and appropriate gestures for given speech. The code, pre-trained models, and demos are available at [here](https://github.com/YoungSeng/DiffuseStyleGesture/tree/DiffuseStyleGesturePlus/BEAT-TWH-main).

[Browse paper here](/files/ICMI_2023_Sicheng_Yang.pdf)

<!-- Paper coming soon -->

