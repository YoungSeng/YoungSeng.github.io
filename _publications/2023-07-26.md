---
title: "UnifiedGesture: A Unified Gesture Synthesis Model for Multiple Skeletons"
collection: publications
permalink: /publication/2023-07-26
date: 2023-07-26
venue: 'ACM MM (Oral)'
citation: '<b>Sicheng Yang*</b>, Zilin Wang*, Zhiyong Wu, Minglei Li, Zhensong Zhang, Qiaochu Huang, Lei Hao, Songcen Xu, Xiaofei Wu, Changpeng Yang, Zonghong Dai. (2023). &quot;UnifiedGesture: A Unified Gesture Synthesis Model for Multiple Skeletons.&quot; <i>ACM MM</i>, 1033â€“1044, doi: 10.1145/3581783.3612503.'
---


<!-- Recommended citation: Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1). -->

The automatic co-speech gesture generation draws much attention in computer animation. Previous works designed network structures on individual datasets, which resulted in a lack of data volume and generalizability across different motion capture standards. In addition, it is a challenging task due to the weak correlation between speech and gestures. To address these problems, we present UnifiedGesture, a novel diffusion model-based speech-driven gesture synthesis approach, trained on multiple gesture datasets with different skeletons. Specifically, we first present a retargeting network to learn latent homeomorphic graphs for different motion capture standards, unifying the representations of various gestures while extending the dataset. We then capture the correlation between speech and gestures based on a diffusion model architecture using cross-local attention and self-attention to generate better speech-matched and realistic gestures. To further align speech and gesture and increase diversity, we incorporate reinforcement learning on the discrete gesture units with a learned reward function. Extensive experiments show that UnifiedGesture outperforms recent approaches on speech-driven gesture generation in terms of CCA, FGD, and human-likeness. All code, pre-trained models, databases, and demos are available to the public at [here](https://github.com/YoungSeng/UnifiedGesture).

[Browse paper here](/files/MM2023_UnifiedGesture.pdf)

<!-- Paper coming soon -->

